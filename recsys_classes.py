import numpy as np
import pandas as pd
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import normalize
from scipy.sparse import csr_matrix, lil_matrix
import logging
import os
from tqdm import tqdm
from surprise import Dataset, Reader, SVD, KNNBaseline
from surprise.model_selection import train_test_split as surprise_train_test_split
import torch

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

class EnhancedKNNRecommender:
    def __init__(self):
        self.item_similarity_matrix = None
        self.user_item_matrix = None
        self.item_id_to_index = {}
        self.user_id_to_index = {}
        self.index_to_item_id = {}
        self.index_to_user_id = {}
    
    def create_sparse_matrix(self, rating_df):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä"""
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä...")
        
        unique_users = sorted(rating_df['user_id'].unique())
        unique_items = sorted(rating_df['product_id'].unique())
        
        self.user_id_to_index = {uid: idx for idx, uid in enumerate(unique_users)}
        self.item_id_to_index = {pid: idx for idx, pid in enumerate(unique_items)}
        self.index_to_user_id = {idx: uid for uid, idx in self.user_id_to_index.items()}
        self.index_to_item_id = {idx: pid for pid, idx in self.item_id_to_index.items()}
        
        n_users = len(unique_users)
        n_items = len(unique_items)
        
        matrix = lil_matrix((n_users, n_items), dtype=np.float32)
        
        for _, row in rating_df.iterrows():
            user_idx = self.user_id_to_index[row['user_id']]
            item_idx = self.item_id_to_index[row['product_id']]
            matrix[user_idx, item_idx] = row['rating']
        
        self.user_item_matrix = matrix.tocsr()
        logger.info(f"–ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: {n_users} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π √ó {n_items} —Ç–æ–≤–∞—Ä–æ–≤")
        return self.user_item_matrix
    
    def compress_dataset(self, rating_df, min_user_interactions=1, min_item_interactions=1):
        """–°–∂–∞—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –±–æ–ª–µ–µ –º—è–≥–∫–∏–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""
        logger.info("–°–∂–∞—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        user_counts = rating_df['user_id'].value_counts()
        item_counts = rating_df['product_id'].value_counts()
        
        # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        active_users = user_counts[user_counts >= min_user_interactions].index
        popular_items = item_counts[item_counts >= min_item_interactions].index
        
        compressed_df = rating_df[
            (rating_df['user_id'].isin(active_users)) & 
            (rating_df['product_id'].isin(popular_items))
        ]
        
        logger.info(f"–°–∂–∞—Ç–∏–µ: {len(rating_df)} ‚Üí {len(compressed_df)} –∑–∞–ø–∏—Å–µ–π "
                f"({len(active_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, {len(popular_items)} —Ç–æ–≤–∞—Ä–æ–≤)")
        
        return compressed_df
        
    def build_similarity_matrix(self, method='cosine', k=50):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–æ–≤–∞—Ä–æ–≤"""
        logger.info(f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ (method={method}, k={k})...")
        
        if self.user_item_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –º–∞—Ç—Ä–∏—Ü—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä")
        
        item_user_matrix = self.user_item_matrix.T
        item_user_matrix_norm = normalize(item_user_matrix, norm='l2', axis=1)
        
        knn = NearestNeighbors(
            n_neighbors=min(k + 1, item_user_matrix_norm.shape[0]),
            metric=method,
            algorithm='brute' if method == 'cosine' else 'auto'
        )
        
        knn.fit(item_user_matrix_norm)
        distances, indices = knn.kneighbors(item_user_matrix_norm)
        
        n_items = item_user_matrix_norm.shape[0]
        similarity_matrix = lil_matrix((n_items, n_items), dtype=np.float32)
        
        for i in range(n_items):
            for j_idx, dist in zip(indices[i], distances[i]):
                if i != j_idx:
                    similarity = 1.0 - dist
                    similarity_matrix[i, j_idx] = similarity
        
        self.item_similarity_matrix = similarity_matrix.tocsr()
        logger.info("–ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
        return self.item_similarity_matrix
    
    def knn_predict(self, user_id, top_n=10):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–æ–º–æ—â—å—é KNN"""
        if user_id not in self.user_id_to_index:
            return None
        
        user_idx = self.user_id_to_index[user_id]
        user_vector = self.user_item_matrix[user_idx].toarray().flatten()
        rated_items = user_vector.nonzero()[0]
        
        if len(rated_items) == 0:
            return None
        
        predictions = np.zeros(self.user_item_matrix.shape[1])
        
        for item_idx in range(self.user_item_matrix.shape[1]):
            if user_vector[item_idx] == 0:
                similar_items = self.item_similarity_matrix[item_idx].indices
                rated_similar_items = np.intersect1d(similar_items, rated_items)
                
                if len(rated_similar_items) > 0:
                    similarities = self.item_similarity_matrix[item_idx, rated_similar_items].toarray().flatten()
                    ratings = user_vector[rated_similar_items]
                    
                    if np.sum(similarities) > 0:
                        predictions[item_idx] = np.sum(similarities * ratings) / np.sum(similarities)
        
        top_indices = np.argsort(predictions)[::-1][:top_n]
        results = []
        
        for idx in top_indices:
            if predictions[idx] > 0:
                results.append({
                    'product_id': self.index_to_item_id[idx],
                    'predicted_rating': predictions[idx],
                    'rank': len(results) + 1
                })
        
        return results
    
    def batch_knn_predict(self, user_ids, top_n=10):
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        all_predictions = []
        
        for user_id in user_ids:
            predictions = self.knn_predict(user_id, top_n)
            if predictions:
                for pred in predictions:
                    all_predictions.append({
                        'user_id': user_id,
                        'product_id': pred['product_id'],
                        'rating_knn': pred['predicted_rating'],
                        'rank': pred['rank']
                    })
        
        return pd.DataFrame(all_predictions)

class AdvancedRatingSystem:
    def __init__(self):
        self.rating_data = {}
        self.feature_weights = {
            'purchase_exists': 2.0,
            'purchase_count': 0.1,
            'reordered': 3.0,
            'morning_purchase': 0.5,
            'evening_purchase': 0.5,
            'global_popularity': 1.0,
            'early_cart_position': 1.0,
            'late_cart_position': -0.3,
            'experimental_user': 0.3,
            'loyal_user': 0.5,
            'time_since_last_order': -0.2,
            'order_size_ratio': 0.4
        }
        self.user_stats = {}
        self.product_stats = {}
    
    def calculate_ratings(self, df):
        logger.info("–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä—ã–π —Ä–∞—Å—á–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ø–æ 12 —Ñ–∏—á–∞–º...")
        
        self._calculate_preliminary_stats(df)
        
        agg_dict = {
            'purchase_count': ('product_id', 'size'),
            'reordered_sum': ('reordered', 'sum'),
            'avg_cart_position': ('add_to_cart_order', 'mean'),
            'avg_time_gap': ('time_since_last_order', 'mean'),
            'avg_size_ratio': ('order_size_ratio', 'mean'),
            'hours': ('order_hour_of_day', lambda x: list(x))
        }
        
        agg_spec = {key: val for key, val in agg_dict.items()}
        user_product_stats = df.groupby(['user_id', 'product_id']).agg(**agg_spec).reset_index()
        
        def calculate_time_features(hours):
            morning = sum(1 for h in hours if 6 <= h <= 12) if hours else 0
            evening = sum(1 for h in hours if 18 <= h <= 24) if hours else 0
            return morning, evening
        
        time_features = user_product_stats['hours'].apply(calculate_time_features)
        user_product_stats['morning_count'] = time_features.apply(lambda x: x[0])
        user_product_stats['evening_count'] = time_features.apply(lambda x: x[1])
        
        user_product_stats = user_product_stats.merge(
            self.user_stats[['user_id', 'unique_products', 'reorder_rate']], 
            on='user_id', how='left'
        )
        
        user_product_stats = user_product_stats.merge(
            self.product_stats[['product_id', 'global_popularity']], 
            on='product_id', how='left'
        )
        
        rating = np.zeros(len(user_product_stats))
        
        rating += self.feature_weights['purchase_exists'] * np.minimum(user_product_stats['purchase_count'], 1)
        rating += self.feature_weights['purchase_count'] * user_product_stats['purchase_count']
        rating += self.feature_weights['reordered'] * np.minimum(user_product_stats['reordered_sum'], 1)
        rating += self.feature_weights['morning_purchase'] * user_product_stats['morning_count']
        rating += self.feature_weights['evening_purchase'] * user_product_stats['evening_count']
        rating += self.feature_weights['global_popularity'] * user_product_stats['global_popularity'].fillna(0)
        rating += np.where(user_product_stats['avg_cart_position'] <= 5, self.feature_weights['early_cart_position'], 0)
        rating += np.where(user_product_stats['avg_cart_position'] >= 15, self.feature_weights['late_cart_position'], 0)
        rating += np.where(user_product_stats['unique_products'] > 50, self.feature_weights['experimental_user'], 0)
        rating += np.where(user_product_stats['reorder_rate'] > 0.7, self.feature_weights['loyal_user'], 0)
        rating += self.feature_weights['time_since_last_order'] * np.minimum(user_product_stats['avg_time_gap'].fillna(0) / 24.0, 1.0)
        rating += self.feature_weights['order_size_ratio'] * user_product_stats['avg_size_ratio'].fillna(1.0)
        
        rating = np.maximum(rating, 0.0)
        
        rating_df = pd.DataFrame({
            'user_id': user_product_stats['user_id'],
            'product_id': user_product_stats['product_id'],
            'rating': rating
        })
        
        logger.info(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–æ–≤: {len(rating_df)} –∑–∞–ø–∏—Å–µ–π")
        return rating_df
    
    def _calculate_preliminary_stats(self, df):
        logger.info("–†–∞—Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        
        self.user_stats = df.groupby('user_id').agg({
            'product_id': 'nunique',
            'reordered': 'mean',
            'order_number': 'max',
            'order_size': 'mean'
        }).reset_index()
        self.user_stats.columns = ['user_id', 'unique_products', 'reorder_rate', 'total_orders', 'avg_order_size']
        
        self.product_stats = df.groupby('product_id').agg({
            'user_id': 'nunique',
            'reordered': 'mean',
            'add_to_cart_order': 'mean'
        }).reset_index()
        self.product_stats.columns = ['product_id', 'unique_users', 'reorder_rate', 'avg_cart_position']
        
        product_popularity = df['product_id'].value_counts()
        max_popularity = product_popularity.max()
        self.product_stats['global_popularity'] = self.product_stats['product_id'].map(
            lambda x: product_popularity.get(x, 0) / max_popularity
        )

class RecommenderOptimizer:
    def __init__(self, rating_df):
        self.rating_df = rating_df
        self.best_knn_params = {'k': 250, 'method': 'cosine'}
        self.best_svd_params = {'n_factors': 480}
        self.best_hybrid_weights = (0.6, 0.4)
        self.data = None
        self.trainset = None
        self.testset = None
        self.knn_recommender = EnhancedKNNRecommender()
        self.compressed_df = None
    
    def prepare_fast_optimization(self):
        """–ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        logger.info("üöÄ –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (10% –¥–∞–Ω–Ω—ã—Ö)
        sample_size = min(500000, len(self.rating_df))
        optimization_df = self.rating_df.sample(sample_size, random_state=42)
        
        # –°–∂–∏–º–∞–µ–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        self.compressed_df = self.knn_recommender.compress_dataset(
            optimization_df, 
            min_user_interactions=1, 
            min_item_interactions=1
        )
        
        logger.info(f"–ü–æ–¥–≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {len(optimization_df)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"–°–∂–∞—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(self.compressed_df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è KNN
        self.knn_recommender.create_sparse_matrix(self.compressed_df)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Surprise
        self.prepare_surprise_data(test_size=0.2)
        
        logger.info("‚úÖ –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    def prepare_surprise_data(self, test_size=0.2):
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è Surprise —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        surprise_sample = self.rating_df.sample(min(100000, len(self.rating_df)), random_state=42)
        
        surprise_df = surprise_sample.rename(columns={
            'user_id': 'userID',
            'product_id': 'itemID'
        })
        
        min_rating = surprise_df['rating'].min()
        max_rating = surprise_df['rating'].max()
        
        reader = Reader(rating_scale=(min_rating, max_rating))
        self.data = Dataset.load_from_df(surprise_df[['userID', 'itemID', 'rating']], reader)
        
        self.trainset, self.testset = surprise_train_test_split(self.data, test_size=test_size, random_state=42)
        return True
        
    def optimize_svd(self, n_factors_list=[80, 120, 200, 360]):
                
        logger.info("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVD –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
        
        best_rmse = float('inf')
        best_n_factors = 50
        
        for n_factors in tqdm(n_factors_list, desc="SVD Optimization"):
            try:
                algo = SVD(n_factors=n_factors, biased=True, random_state=999, verbose=False)
                
                algo.fit(self.trainset)
                predictions = algo.test(self.testset)
                
                rmse = np.sqrt(np.mean([(pred.r_ui - pred.est) ** 2 for pred in predictions]))
                
                if rmse < best_rmse:
                    best_rmse = rmse
                    best_n_factors = n_factors
                    
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –¥–ª—è n_factors={n_factors}: {e}")
                continue
        
        self.best_svd_params = {'n_factors': best_n_factors}
        logger.info(f"üéØ –õ—É—á—à–∏–µ SVD –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: n_factors={best_n_factors}, RMSE={best_rmse:.4f}")
        
        return best_n_factors, best_rmse

    def optimize_knn_on_compressed_data(self, k_list=[20, 50, 100]):
        """KNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∂–∞—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        logger.info("KNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∂–∞—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...")
        
        if self.compressed_df is None:
            self.compressed_df = self.knn_recommender.compress_dataset(
                self.rating_df, min_user_interactions=1, min_item_interactions=1
            )
        
        self.knn_recommender.create_sparse_matrix(self.compressed_df)
        
        best_rmse = float('inf')
        best_k = 50
        
        for k in tqdm(k_list, desc="KNN Optimization on compressed data"):
            try:
                self.knn_recommender.build_similarity_matrix(k=k)
                
                # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ
                sample_users = self.compressed_df['user_id'].unique()[:50]
                knn_predictions = self.knn_recommender.batch_knn_predict(sample_users, 10)
                
                if len(knn_predictions) > 0:
                    rmse = self._evaluate_knn_predictions(knn_predictions)
                    
                    if rmse < best_rmse and rmse != float('inf'):
                        best_rmse = rmse
                        best_k = k
                        
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –¥–ª—è k={k}: {e}")
                continue
        
        self.best_knn_params['k'] = best_k
        logger.info(f"üéØ –õ—É—á—à–∏–π KNN: k={best_k}, RMSE={best_rmse:.4f}")
        return best_k, best_rmse

    def _evaluate_knn_predictions(self, knn_predictions):
        """–û—Ü–µ–Ω–∫–∞ KNN –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        if len(knn_predictions) == 0:
            return float('inf')
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º inner join –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        merged = knn_predictions.merge(
            self.rating_df, 
            on=['user_id', 'product_id'],
            how='inner'
        )
        
        if len(merged) == 0:
            return float('inf')
        
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –µ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        valid_data = merged.dropna(subset=['rating', 'rating_knn'])
        if len(valid_data) == 0:
            return float('inf')
        
        return np.sqrt(mean_squared_error(valid_data['rating'], valid_data['rating_knn']))

    def optimize_hybrid_weights(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        logger.info("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –≤–µ—Å–æ–≤...")
        
        best_score = 0
        best_weights = (0.6, 0.4)  # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ä–∞–≤–Ω—ã—Ö –≤–µ—Å–æ–≤
        
        for svd_weight in [0.4, 0.5, 0.6, 0.7]:
            knn_weight = 1.0 - svd_weight
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏
            sample_users = self.compressed_df['user_id'].unique()[:20]
            hybrid_score = self._evaluate_hybrid_weights(sample_users, svd_weight, knn_weight)
            
            if hybrid_score > best_score:
                best_score = hybrid_score
                best_weights = (svd_weight, knn_weight)
        
        self.best_hybrid_weights = best_weights
        logger.info(f"üéØ –õ—É—á—à–∏–µ –≤–µ—Å–∞: SVD={best_weights[0]}, KNN={best_weights[1]}")
        return best_weights

    def _evaluate_hybrid_weights(self, user_ids, svd_weight, knn_weight):
        """–û—Ü–µ–Ω–∫–∞ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –≤–µ—Å–æ–≤"""
        _, svd_algo = self.train_final_models()
        knn_predictions = self.knn_recommender.batch_knn_predict(user_ids, 10)
        
        scores = []
        
        for user_id in user_ids:
            try:
                user_inner_id = svd_algo.trainset.to_inner_uid(str(user_id))
                user_bias = svd_algo.bu[user_inner_id]
                user_factors = svd_algo.pu[user_inner_id]
                svd_ratings = svd_algo.bi + user_bias + np.dot(svd_algo.qi, user_factors)
                
                user_knn = knn_predictions[knn_predictions['user_id'] == user_id]
                
                hybrid_scores = []
                for item_inner_id, svd_rating in enumerate(svd_ratings):
                    try:
                        item_id = int(svd_algo.trainset.to_raw_iid(item_inner_id))
                        knn_rating = 0
                        
                        knn_match = user_knn[user_knn['product_id'] == item_id]
                        if not knn_match.empty:
                            knn_rating = knn_match['rating_knn'].values[0]
                        
                        hybrid_rating = (svd_weight * svd_rating + 
                                       knn_weight * knn_rating if knn_rating > 0 else svd_rating)
                        hybrid_scores.append(hybrid_rating)
                        
                    except:
                        continue
                
                if hybrid_scores:
                    scores.append(max(hybrid_scores))
                    
            except:
                continue
        
        return np.mean(scores) if scores else 0

    def train_final_models(self):
                  
        logger.info("–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π SVD –º–æ–¥–µ–ª–∏...")
        
        full_trainset = self.data.build_full_trainset()
        
        svd_algo = SVD(n_factors=self.best_svd_params['n_factors'], biased=True, 
                      random_state=999, verbose=False)
        svd_algo.fit(full_trainset)
        
        return None, svd_algo
    
    def create_simple_submission(self, n_recommendations=10):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ submission —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤"""
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...")
        
        # –ü—Ä–æ—Å—Ç–æ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        submission_df = (
            self.rating_df
            .sort_values(['user_id', 'rating'], ascending=[True, False])
            .groupby('user_id')
            .head(n_recommendations)
            .groupby('user_id')['product_id']
            .apply(lambda x: ' '.join(map(str, x)))
            .reset_index()
            .rename(columns={'product_id': 'product_id'})
        )
        
        submission_df.to_csv('submission.csv', index=False)
        logger.info(f"‚úÖ Submission —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {len(submission_df)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        
        return True

class OurDataWinnerV20:
    def __init__(self):
        self.rating_system = AdvancedRatingSystem()
        self.rating_df = None
        self.model = None
        self.user_ids = None
        self.product_ids = None
        self.user_id_to_idx = {}
        self.product_to_idx = {}
        self.data_files = {}
        self.column_mapping = {}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.recommender_optimizer = None
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
    
    def auto_detect_data_files(self):
        logger.info("–ê–≤—Ç–æ–ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
        
        possible_files = {
            'transactions': [
                'transactions.csv'
            ]
        }
        
        detected_files = {}
        
        for data_type, filenames in possible_files.items():
            for filename in filenames:
                if os.path.exists(filename):
                    detected_files[data_type] = filename
                    logger.info(f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª {data_type}: {filename}")
                    break
                data_path = f"data/{filename}"
                if os.path.exists(data_path):
                    detected_files[data_type] = data_path
                    logger.info(f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª {data_type}: {data_path}")
                    break
        
        self.data_files = detected_files
        return detected_files
    
    def detect_column_mapping(self, file_path):
        logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è {file_path}...")
        
        try:
            sample = pd.read_csv(file_path, nrows=1000)
            columns = sample.columns.tolist()
            
            column_patterns = {
                'user_id': ['user_id', 'userid', 'user', 'customer_id', 'customerid', 'customer', 'uid'],
                'product_id': ['product_id', 'productid', 'product', 'item_id', 'itemid', 'item', 'pid'],
                'order_id': ['order_id', 'orderid', 'order', 'basket_id', 'basketid', 'order_number'],
                'order_number': ['order_number', 'ordernumber', 'order_seq', 'sequence', 'order_sequence'],
                'reordered': ['reordered', 'reorder', 'repeat', 'repurchased'],
                'add_to_cart_order': ['add_to_cart_order', 'cart_order', 'add_order'],
                'order_dow': ['order_dow', 'day_of_week', 'dow', 'weekday'],
                'order_hour_of_day': ['order_hour_of_day', 'hour_of_day', 'hour', 'order_hour']
            }
            
            mapping = {}
            for standard_name, patterns in column_patterns.items():
                for pattern in patterns:
                    if pattern in columns:
                        mapping[standard_name] = pattern
                        logger.info(f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ {standard_name} -> {pattern}")
                        break
            
            if 'user_id' not in mapping and len(columns) > 0:
                mapping['user_id'] = columns[0]
            if 'product_id' not in mapping and len(columns) > 1:
                mapping['product_id'] = columns[1]
            
            self.column_mapping = mapping
            logger.info(f"–§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫: {mapping}")
            return mapping
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–ª–æ–Ω–æ–∫: {e}")
            return {}
    
    def load_and_preprocess_data(self):
        logger.info("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        if not self.data_files.get('transactions'):
            logger.error("–î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            return False
        
        try:
            self.column_mapping = self.detect_column_mapping(self.data_files['transactions'])
            
            if not self.column_mapping:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫!")
                return False
            
            usecols = list(self.column_mapping.values())
            dtype = {}
            
            for std_col, actual_col in self.column_mapping.items():
                if std_col in ['user_id', 'product_id', 'order_id']:
                    dtype[actual_col] = 'int32'
                elif std_col in ['order_number', 'add_to_cart_order', 'order_dow', 'order_hour_of_day']:
                    dtype[actual_col] = 'int16'
                elif std_col == 'reordered':
                    dtype[actual_col] = 'int8'
            
            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            transactions = pd.read_csv(
                self.data_files['transactions'],
                usecols=usecols,
                dtype=dtype
            )
            
            reverse_mapping = {v: k for k, v in self.column_mapping.items()}
            transactions = transactions.rename(columns=reverse_mapping)
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(transactions):,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
            
            transactions = self.optimize_data_types(transactions)
            transactions = self.add_temporal_features(transactions)
            self.create_mappings(transactions)
            self.rating_df = self.rating_system.calculate_ratings(transactions)
            
            os.makedirs('processed_data', exist_ok=True)
            transactions.to_parquet('processed_data/transactions_processed.parquet', index=False)
            self.rating_df.to_parquet('processed_data/ratings_dataset.parquet', index=False)
            
            logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(self.rating_df)} –∑–∞–ø–∏—Å–µ–π")
            
            self._show_rating_stats()
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def optimize_data_types(self, df):
        dtype_optimization = {
            'user_id': 'int32',
            'product_id': 'int32', 
            'order_id': 'int32',
            'order_number': 'int16',
            'order_dow': 'int8',
            'order_hour_of_day': 'int8',
            'reordered': 'int8',
            'add_to_cart_order': 'int16'
        }
        
        for col, target_dtype in dtype_optimization.items():
            if col in df.columns:
                df[col] = df[col].astype(target_dtype)
        
        return df
    
    def add_temporal_features(self, df):
        logger.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∏—á...")
        
        try:
            if 'order_number' in df.columns:
                df = df.sort_values(['user_id', 'order_number'])
                df['time_since_last_order'] = df.groupby('user_id')['order_number'].diff().fillna(0)
            
            if 'order_id' in df.columns:
                order_sizes = df.groupby(['user_id', 'order_id']).size().reset_index(name='order_size')
                df = df.merge(order_sizes, on=['user_id', 'order_id'], how='left')
                
                user_avg_order = df.groupby('user_id')['order_size'].mean().reset_index(name='avg_user_order_size')
                df = df.merge(user_avg_order, on='user_id', how='left')
                df['order_size_ratio'] = df['order_size'] / df['avg_user_order_size'].replace(0, 1)
            
            logger.info("‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã")
            return df
            
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏: {e}")
            return df
    
    def create_mappings(self, df):
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤...")
        
        self.user_ids = df['user_id'].unique()
        self.product_ids = df['product_id'].unique()
        
        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(self.user_ids)}
        self.product_to_idx = {pid: idx for idx, pid in enumerate(self.product_ids)}
        
        logger.info(f"‚úÖ –ú–∞–ø–ø–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã: {len(self.user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, {len(self.product_ids)} —Ç–æ–≤–∞—Ä–æ–≤")
    
    def _show_rating_stats(self):
        if self.rating_df is not None:
            logger.info("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤:")
            logger.info(f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.rating_df):,}")
            logger.info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {self.rating_df['user_id'].nunique()}")
            logger.info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {self.rating_df['product_id'].nunique()}")
            logger.info(f"  –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {self.rating_df['rating'].mean():.3f}")
            logger.info(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {self.rating_df['rating'].min():.3f}")
            logger.info(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {self.rating_df['rating'].max():.3f}")
            logger.info(f"  –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {self.rating_df['rating'].median():.3f}")
    
    def optimize_recommendations(self, n_recommendations=10):
        logger.info("üéØ –ë—ã—Å—Ç—Ä–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVD + KNN...")
        
        if self.rating_df is None:
            logger.error("–†–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω!")
            return False
        
        self.recommender_optimizer = RecommenderOptimizer(self.rating_df)
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞
        self.recommender_optimizer.prepare_fast_optimization()
        
       
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVD
        best_n_factors, svd_rmse = self.recommender_optimizer.optimize_svd()
        self.recommender_optimizer.best_svd_params = {'n_factors': best_n_factors}
        
        # –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è KNN
        best_k, knn_rmse = self.recommender_optimizer.optimize_knn_on_compressed_data()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –≤–µ—Å–æ–≤
        best_weights = self.recommender_optimizer.optimize_hybrid_weights()
        
        logger.info(f"‚úÖ –ì–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        logger.info(f"   SVD: n_factors={best_n_factors}, RMSE={svd_rmse:.4f}")
        logger.info(f"   KNN: k={best_k}, RMSE={knn_rmse:.4f}")
        logger.info(f"   –í–µ—Å–∞: SVD={best_weights[0]}, KNN={best_weights[1]}")
            
        
        
        return True
    
    def create_submission_file(self, n_recommendations=10):
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è submission —Ñ–∞–π–ª–∞
        if self.recommender_optimizer is not None:
            success = self.recommender_optimizer.create_simple_submission(n_recommendations)
        else:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π submission –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
            submission_df = (
                self.rating_df
                .sort_values(['user_id', 'rating'], ascending=[True, False])
                .groupby('user_id')
                .head(n_recommendations)
                .groupby('user_id')['product_id']
                .apply(lambda x: ' '.join(map(str, x)))
                .reset_index()
                .rename(columns={'product_id': 'products'})
            )
            
            submission_df.to_csv('submission.csv', index=False)
            success = True
        
        if success:
            logger.info("‚úÖ Submission —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
            return True
        else:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ submission —Ñ–∞–π–ª–∞!")
            return False


def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ OurDataWinnerV20...")
    
    recommender = OurDataWinnerV20()
    
    # –ê–≤—Ç–æ–ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
    recommender.auto_detect_data_files()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if not recommender.load_and_preprocess_data():
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return
    
    # –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    if not recommender.optimize_recommendations():
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏!")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞
    if not recommender.create_submission_file():
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å submission —Ñ–∞–π–ª!")
        return
    
    logger.info("üéâ –ü—Ä–æ–≥—Ä–∞–º–º–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    main()