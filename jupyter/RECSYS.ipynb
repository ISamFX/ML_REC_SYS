{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f349686a-f336-49a8-877a-4e213a18120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from surprise import Dataset, Reader, KNNBaseline, SVD\n",
    "from surprise.model_selection import cross_validate, train_test_split as surprise_train_test_split\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a558312-baac-46dd-83c9-a18d7cb022f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataWinnerV20:\n",
    "    def __init__(self):\n",
    "        self.rating_system = AdvancedRatingSystem()\n",
    "        self.rating_df = None\n",
    "        self.model = None\n",
    "        self.user_ids = None\n",
    "        self.product_ids = None\n",
    "        self.user_id_to_idx = {}\n",
    "        self.product_to_idx = {}\n",
    "        self.data_files = {}\n",
    "        self.column_mapping = {}\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.recommender_optimizer = None\n",
    "        logger.info(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}\")\n",
    "    \n",
    "    def auto_detect_data_files(self):\n",
    "        logger.info(\"–ê–≤—Ç–æ–ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "        \n",
    "        possible_files = {\n",
    "            'transactions': [\n",
    "                'transactions.csv'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        detected_files = {}\n",
    "        \n",
    "        for data_type, filenames in possible_files.items():\n",
    "            for filename in filenames:\n",
    "                if os.path.exists(filename):\n",
    "                    detected_files[data_type] = filename\n",
    "                    logger.info(f\"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª {data_type}: {filename}\")\n",
    "                    break\n",
    "                data_path = f\"data/{filename}\"\n",
    "                if os.path.exists(data_path):\n",
    "                    detected_files[data_type] = data_path\n",
    "                    logger.info(f\"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª {data_type}: {data_path}\")\n",
    "                    break\n",
    "        \n",
    "        self.data_files = detected_files\n",
    "        return detected_files\n",
    "    \n",
    "    def detect_column_mapping(self, file_path):\n",
    "        logger.info(f\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è {file_path}...\")\n",
    "        \n",
    "        try:\n",
    "            sample = pd.read_csv(file_path, nrows=1000)\n",
    "            columns = sample.columns.tolist()\n",
    "            \n",
    "            column_patterns = {\n",
    "                'user_id': ['user_id', 'userid', 'user', 'customer_id', 'customerid', 'customer', 'uid'],\n",
    "                'product_id': ['product_id', 'productid', 'product', 'item_id', 'itemid', 'item', 'pid'],\n",
    "                'order_id': ['order_id', 'orderid', 'order', 'basket_id', 'basketid', 'order_number'],\n",
    "                'order_number': ['order_number', 'ordernumber', 'order_seq', 'sequence', 'order_sequence'],\n",
    "                'reordered': ['reordered', 'reorder', 'repeat', 'repurchased'],\n",
    "                'add_to_cart_order': ['add_to_cart_order', 'cart_order', 'add_order'],\n",
    "                'order_dow': ['order_dow', 'day_of_week', 'dow', 'weekday'],\n",
    "                'order_hour_of_day': ['order_hour_of_day', 'hour_of_day', 'hour', 'order_hour']\n",
    "            }\n",
    "            \n",
    "            mapping = {}\n",
    "            for standard_name, patterns in column_patterns.items():\n",
    "                for pattern in patterns:\n",
    "                    if pattern in columns:\n",
    "                        mapping[standard_name] = pattern\n",
    "                        logger.info(f\"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ {standard_name} -> {pattern}\")\n",
    "                        break\n",
    "            \n",
    "            if 'user_id' not in mapping and len(columns) > 0:\n",
    "                mapping['user_id'] = columns[0]\n",
    "            if 'product_id' not in mapping and len(columns) > 1:\n",
    "                mapping['product_id'] = columns[1]\n",
    "            \n",
    "            self.column_mapping = mapping\n",
    "            logger.info(f\"–§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫: {mapping}\")\n",
    "            return mapping\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–ª–æ–Ω–æ–∫: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def load_and_preprocess_data(self):\n",
    "        logger.info(\"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "        \n",
    "        if not self.data_files.get('transactions'):\n",
    "            logger.error(\"–î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self.column_mapping = self.detect_column_mapping(self.data_files['transactions'])\n",
    "            \n",
    "            if not self.column_mapping:\n",
    "                logger.error(\"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫!\")\n",
    "                return False\n",
    "            \n",
    "            usecols = list(self.column_mapping.values())\n",
    "            dtype = {}\n",
    "            \n",
    "            for std_col, actual_col in self.column_mapping.items():\n",
    "                if std_col in ['user_id', 'product_id', 'order_id']:\n",
    "                    dtype[actual_col] = 'int32'\n",
    "                elif std_col in ['order_number', 'add_to_cart_order', 'order_dow', 'order_hour_of_day']:\n",
    "                    dtype[actual_col] = 'int16'\n",
    "                elif std_col == 'reordered':\n",
    "                    dtype[actual_col] = 'int8'\n",
    "            \n",
    "            logger.info(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "            transactions = pd.read_csv(\n",
    "                self.data_files['transactions'],\n",
    "                usecols=usecols,\n",
    "                dtype=dtype\n",
    "            )\n",
    "            \n",
    "            reverse_mapping = {v: k for k, v in self.column_mapping.items()}\n",
    "            transactions = transactions.rename(columns=reverse_mapping)\n",
    "            \n",
    "            logger.info(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(transactions):,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π\")\n",
    "            \n",
    "            transactions = self.optimize_data_types(transactions)\n",
    "            transactions = self.add_temporal_features(transactions)\n",
    "            self.create_mappings(transactions)\n",
    "            self.rating_df = self.rating_system.calculate_ratings(transactions)\n",
    "            \n",
    "            os.makedirs('processed_data', exist_ok=True)\n",
    "            transactions.to_parquet('processed_data/transactions_processed.parquet', index=False)\n",
    "            self.rating_df.to_parquet('processed_data/ratings_dataset.parquet', index=False)\n",
    "            \n",
    "            logger.info(\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\")\n",
    "            logger.info(f\"‚úÖ –°–æ–∑–¥–∞–Ω —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(self.rating_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "            \n",
    "            self._show_rating_stats()\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def optimize_data_types(self, df):\n",
    "        dtype_optimization = {\n",
    "            'user_id': 'int32',\n",
    "            'product_id': 'int32', \n",
    "            'order_id': 'int32',\n",
    "            'order_number': 'int16',\n",
    "            'order_dow': 'int8',\n",
    "            'order_hour_of_day': 'int8',\n",
    "            'reordered': 'int8',\n",
    "            'add_to_cart_order': 'int16'\n",
    "        }\n",
    "        \n",
    "        for col, target_dtype in dtype_optimization.items():\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(target_dtype)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_temporal_features(self, df):\n",
    "        logger.info(\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∏—á...\")\n",
    "        \n",
    "        try:\n",
    "            if 'order_number' in df.columns:\n",
    "                df = df.sort_values(['user_id', 'order_number'])\n",
    "                df['time_since_last_order'] = df.groupby('user_id')['order_number'].diff().fillna(0)\n",
    "            \n",
    "            if 'order_id' in df.columns:\n",
    "                order_sizes = df.groupby(['user_id', 'order_id']).size().reset_index(name='order_size')\n",
    "                df = df.merge(order_sizes, on=['user_id', 'order_id'], how='left')\n",
    "                \n",
    "                user_avg_order = df.groupby('user_id')['order_size'].mean().reset_index(name='avg_user_order_size')\n",
    "                df = df.merge(user_avg_order, on='user_id', how='left')\n",
    "                df['order_size_ratio'] = df['order_size'] / df['avg_user_order_size'].replace(0, 1)\n",
    "            \n",
    "            logger.info(\"‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def create_mappings(self, df):\n",
    "        logger.info(\"–°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤...\")\n",
    "        \n",
    "        self.user_ids = df['user_id'].unique()\n",
    "        self.product_ids = df['product_id'].unique()\n",
    "        \n",
    "        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(self.user_ids)}\n",
    "        self.product_to_idx = {pid: idx for idx, pid in enumerate(self.product_ids)}\n",
    "        \n",
    "        logger.info(f\"‚úÖ –ú–∞–ø–ø–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã: {len(self.user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, {len(self.product_ids)} —Ç–æ–≤–∞—Ä–æ–≤\")\n",
    "    \n",
    "    def _show_rating_stats(self):\n",
    "        if self.rating_df is not None:\n",
    "            logger.info(\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤:\")\n",
    "            logger.info(f\"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.rating_df):,}\")\n",
    "            logger.info(f\"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {self.rating_df['user_id'].nunique()}\")\n",
    "            logger.info(f\"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {self.rating_df['product_id'].nunique()}\")\n",
    "            logger.info(f\"  –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {self.rating_df['rating'].mean():.3f}\")\n",
    "            logger.info(f\"  –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {self.rating_df['rating'].min():.3f}\")\n",
    "            logger.info(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {self.rating_df['rating'].max():.3f}\")\n",
    "            logger.info(f\"  –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {self.rating_df['rating'].median():.3f}\")\n",
    "    \n",
    "    def optimize_recommendations(self, n_recommendations=10):\n",
    "        logger.info(\"üéØ –ë—ã—Å—Ç—Ä–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVD + KNN...\")\n",
    "        \n",
    "        if self.rating_df is None:\n",
    "            logger.error(\"–†–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω!\")\n",
    "            return False\n",
    "        \n",
    "        self.recommender_optimizer = RecommenderOptimizer(self.rating_df)\n",
    "        \n",
    "        # –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n",
    "        self.recommender_optimizer.prepare_fast_optimization()\n",
    "        \n",
    "       \n",
    "        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVD\n",
    "        best_n_factors, svd_rmse = self.recommender_optimizer.optimize_svd()\n",
    "        self.recommender_optimizer.best_svd_params = {'n_factors': best_n_factors}\n",
    "        \n",
    "        # –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è KNN\n",
    "        best_k, knn_rmse = self.recommender_optimizer.optimize_knn_on_compressed_data()\n",
    "        \n",
    "        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –≤–µ—Å–æ–≤\n",
    "        best_weights = self.recommender_optimizer.optimize_hybrid_weights()\n",
    "        \n",
    "        logger.info(f\"‚úÖ –ì–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "        logger.info(f\"   SVD: n_factors={best_n_factors}, RMSE={svd_rmse:.4f}\")\n",
    "        logger.info(f\"   KNN: k={best_k}, RMSE={knn_rmse:.4f}\")\n",
    "        logger.info(f\"   –í–µ—Å–∞: SVD={best_weights[0]}, KNN={best_weights[1]}\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def create_submission_file(self, n_recommendations=10):\n",
    "        logger.info(\"–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...\")\n",
    "        \n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è submission —Ñ–∞–π–ª–∞\n",
    "        if self.recommender_optimizer is not None:\n",
    "            success = self.recommender_optimizer.create_simple_submission(n_recommendations)\n",
    "        else:\n",
    "            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π submission –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤\n",
    "            submission_df = (\n",
    "                self.rating_df\n",
    "                .sort_values(['user_id', 'rating'], ascending=[True, False])\n",
    "                .groupby('user_id')\n",
    "                .head(n_recommendations)\n",
    "                .groupby('user_id')['product_id']\n",
    "                .apply(lambda x: ' '.join(map(str, x)))\n",
    "                .reset_index()\n",
    "                .rename(columns={'product_id': 'products'})\n",
    "            )\n",
    "            \n",
    "            submission_df.to_csv('submission.csv', index=False)\n",
    "            success = True\n",
    "        \n",
    "        if success:\n",
    "            logger.info(\"‚úÖ Submission —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ submission —Ñ–∞–π–ª–∞!\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4c2de9-c028-44b4-8063-c627eec72dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRatingSystem:\n",
    "    def __init__(self):\n",
    "        self.rating_data = {}\n",
    "        self.feature_weights = {\n",
    "            'purchase_exists': 2.0,\n",
    "            'purchase_count': 0.1,\n",
    "            'reordered': 3.0,\n",
    "            'morning_purchase': 0.5,\n",
    "            'evening_purchase': 0.5,\n",
    "            'global_popularity': 1.0,\n",
    "            'early_cart_position': 1.0,\n",
    "            'late_cart_position': -0.3,\n",
    "            'experimental_user': 0.3,\n",
    "            'loyal_user': 0.5,\n",
    "            'time_since_last_order': -0.2,\n",
    "            'order_size_ratio': 0.4\n",
    "        }\n",
    "        self.user_stats = {}\n",
    "        self.product_stats = {}\n",
    "    \n",
    "    def calculate_ratings(self, df):\n",
    "        logger.info(\"–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä—ã–π —Ä–∞—Å—á–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ø–æ 12 —Ñ–∏—á–∞–º...\")\n",
    "        \n",
    "        self._calculate_preliminary_stats(df)\n",
    "        \n",
    "        agg_dict = {\n",
    "            'purchase_count': ('product_id', 'size'),\n",
    "            'reordered_sum': ('reordered', 'sum'),\n",
    "            'avg_cart_position': ('add_to_cart_order', 'mean'),\n",
    "            'avg_time_gap': ('time_since_last_order', 'mean'),\n",
    "            'avg_size_ratio': ('order_size_ratio', 'mean'),\n",
    "            'hours': ('order_hour_of_day', lambda x: list(x))\n",
    "        }\n",
    "        \n",
    "        agg_spec = {key: val for key, val in agg_dict.items()}\n",
    "        user_product_stats = df.groupby(['user_id', 'product_id']).agg(**agg_spec).reset_index()\n",
    "        \n",
    "        def calculate_time_features(hours):\n",
    "            morning = sum(1 for h in hours if 6 <= h <= 12) if hours else 0\n",
    "            evening = sum(1 for h in hours if 18 <= h <= 24) if hours else 0\n",
    "            return morning, evening\n",
    "        \n",
    "        time_features = user_product_stats['hours'].apply(calculate_time_features)\n",
    "        user_product_stats['morning_count'] = time_features.apply(lambda x: x[0])\n",
    "        user_product_stats['evening_count'] = time_features.apply(lambda x: x[1])\n",
    "        \n",
    "        user_product_stats = user_product_stats.merge(\n",
    "            self.user_stats[['user_id', 'unique_products', 'reorder_rate']], \n",
    "            on='user_id', how='left'\n",
    "        )\n",
    "        \n",
    "        user_product_stats = user_product_stats.merge(\n",
    "            self.product_stats[['product_id', 'global_popularity']], \n",
    "            on='product_id', how='left'\n",
    "        )\n",
    "        \n",
    "        rating = np.zeros(len(user_product_stats))\n",
    "        \n",
    "        rating += self.feature_weights['purchase_exists'] * np.minimum(user_product_stats['purchase_count'], 1)\n",
    "        rating += self.feature_weights['purchase_count'] * user_product_stats['purchase_count']\n",
    "        rating += self.feature_weights['reordered'] * np.minimum(user_product_stats['reordered_sum'], 1)\n",
    "        rating += self.feature_weights['morning_purchase'] * user_product_stats['morning_count']\n",
    "        rating += self.feature_weights['evening_purchase'] * user_product_stats['evening_count']\n",
    "        rating += self.feature_weights['global_popularity'] * user_product_stats['global_popularity'].fillna(0)\n",
    "        rating += np.where(user_product_stats['avg_cart_position'] <= 5, self.feature_weights['early_cart_position'], 0)\n",
    "        rating += np.where(user_product_stats['avg_cart_position'] >= 15, self.feature_weights['late_cart_position'], 0)\n",
    "        rating += np.where(user_product_stats['unique_products'] > 50, self.feature_weights['experimental_user'], 0)\n",
    "        rating += np.where(user_product_stats['reorder_rate'] > 0.7, self.feature_weights['loyal_user'], 0)\n",
    "        rating += self.feature_weights['time_since_last_order'] * np.minimum(user_product_stats['avg_time_gap'].fillna(0) / 24.0, 1.0)\n",
    "        rating += self.feature_weights['order_size_ratio'] * user_product_stats['avg_size_ratio'].fillna(1.0)\n",
    "        \n",
    "        rating = np.maximum(rating, 0.0)\n",
    "        \n",
    "        rating_df = pd.DataFrame({\n",
    "            'user_id': user_product_stats['user_id'],\n",
    "            'product_id': user_product_stats['product_id'],\n",
    "            'rating': rating\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–æ–≤: {len(rating_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "        return rating_df\n",
    "    \n",
    "    def _calculate_preliminary_stats(self, df):\n",
    "        logger.info(\"–†–∞—Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\")\n",
    "        \n",
    "        self.user_stats = df.groupby('user_id').agg({\n",
    "            'product_id': 'nunique',\n",
    "            'reordered': 'mean',\n",
    "            'order_number': 'max',\n",
    "            'order_size': 'mean'\n",
    "        }).reset_index()\n",
    "        self.user_stats.columns = ['user_id', 'unique_products', 'reorder_rate', 'total_orders', 'avg_order_size']\n",
    "        \n",
    "        self.product_stats = df.groupby('product_id').agg({\n",
    "            'user_id': 'nunique',\n",
    "            'reordered': 'mean',\n",
    "            'add_to_cart_order': 'mean'\n",
    "        }).reset_index()\n",
    "        self.product_stats.columns = ['product_id', 'unique_users', 'reorder_rate', 'avg_cart_position']\n",
    "        \n",
    "        product_popularity = df['product_id'].value_counts()\n",
    "        max_popularity = product_popularity.max()\n",
    "        self.product_stats['global_popularity'] = self.product_stats['product_id'].map(\n",
    "            lambda x: product_popularity.get(x, 0) / max_popularity\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3602a0ce-8bc8-4f84-b989-6c783f66f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedKNNRecommender:\n",
    "    def __init__(self):\n",
    "        self.item_similarity_matrix = None\n",
    "        self.user_item_matrix = None\n",
    "        self.item_id_to_index = {}\n",
    "        self.user_id_to_index = {}\n",
    "        self.index_to_item_id = {}\n",
    "        self.index_to_user_id = {}\n",
    "    \n",
    "    def create_sparse_matrix(self, rating_df):\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä\"\"\"\n",
    "        logger.info(\"–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä...\")\n",
    "        \n",
    "        unique_users = sorted(rating_df['user_id'].unique())\n",
    "        unique_items = sorted(rating_df['product_id'].unique())\n",
    "        \n",
    "        self.user_id_to_index = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "        self.item_id_to_index = {pid: idx for idx, pid in enumerate(unique_items)}\n",
    "        self.index_to_user_id = {idx: uid for uid, idx in self.user_id_to_index.items()}\n",
    "        self.index_to_item_id = {idx: pid for pid, idx in self.item_id_to_index.items()}\n",
    "        \n",
    "        n_users = len(unique_users)\n",
    "        n_items = len(unique_items)\n",
    "        \n",
    "        matrix = lil_matrix((n_users, n_items), dtype=np.float32)\n",
    "        \n",
    "        for _, row in rating_df.iterrows():\n",
    "            user_idx = self.user_id_to_index[row['user_id']]\n",
    "            item_idx = self.item_id_to_index[row['product_id']]\n",
    "            matrix[user_idx, item_idx] = row['rating']\n",
    "        \n",
    "        self.user_item_matrix = matrix.tocsr()\n",
    "        logger.info(f\"–ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: {n_users} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π √ó {n_items} —Ç–æ–≤–∞—Ä–æ–≤\")\n",
    "        return self.user_item_matrix\n",
    "    \n",
    "    def compress_dataset(self, rating_df, min_user_interactions=1, min_item_interactions=1):\n",
    "        \"\"\"–°–∂–∞—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –±–æ–ª–µ–µ –º—è–≥–∫–∏–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏\"\"\"\n",
    "        logger.info(\"–°–∂–∞—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
    "        \n",
    "        user_counts = rating_df['user_id'].value_counts()\n",
    "        item_counts = rating_df['product_id'].value_counts()\n",
    "        \n",
    "        # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "        active_users = user_counts[user_counts >= min_user_interactions].index\n",
    "        popular_items = item_counts[item_counts >= min_item_interactions].index\n",
    "        \n",
    "        compressed_df = rating_df[\n",
    "            (rating_df['user_id'].isin(active_users)) & \n",
    "            (rating_df['product_id'].isin(popular_items))\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"–°–∂–∞—Ç–∏–µ: {len(rating_df)} ‚Üí {len(compressed_df)} –∑–∞–ø–∏—Å–µ–π \"\n",
    "                f\"({len(active_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, {len(popular_items)} —Ç–æ–≤–∞—Ä–æ–≤)\")\n",
    "        \n",
    "        return compressed_df\n",
    "        \n",
    "    def build_similarity_matrix(self, method='cosine', k=50):\n",
    "        \"\"\"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–æ–≤–∞—Ä–æ–≤\"\"\"\n",
    "        logger.info(f\"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ (method={method}, k={k})...\")\n",
    "        \n",
    "        if self.user_item_matrix is None:\n",
    "            raise ValueError(\"–°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –º–∞—Ç—Ä–∏—Ü—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä\")\n",
    "        \n",
    "        item_user_matrix = self.user_item_matrix.T\n",
    "        item_user_matrix_norm = normalize(item_user_matrix, norm='l2', axis=1)\n",
    "        \n",
    "        knn = NearestNeighbors(\n",
    "            n_neighbors=min(k + 1, item_user_matrix_norm.shape[0]),\n",
    "            metric=method,\n",
    "            algorithm='brute' if method == 'cosine' else 'auto'\n",
    "        )\n",
    "        \n",
    "        knn.fit(item_user_matrix_norm)\n",
    "        distances, indices = knn.kneighbors(item_user_matrix_norm)\n",
    "        \n",
    "        n_items = item_user_matrix_norm.shape[0]\n",
    "        similarity_matrix = lil_matrix((n_items, n_items), dtype=np.float32)\n",
    "        \n",
    "        for i in range(n_items):\n",
    "            for j_idx, dist in zip(indices[i], distances[i]):\n",
    "                if i != j_idx:\n",
    "                    similarity = 1.0 - dist\n",
    "                    similarity_matrix[i, j_idx] = similarity\n",
    "        \n",
    "        self.item_similarity_matrix = similarity_matrix.tocsr()\n",
    "        logger.info(\"–ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞\")\n",
    "        return self.item_similarity_matrix\n",
    "    \n",
    "    def knn_predict(self, user_id, top_n=10):\n",
    "        \"\"\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–æ–º–æ—â—å—é KNN\"\"\"\n",
    "        if user_id not in self.user_id_to_index:\n",
    "            return None\n",
    "        \n",
    "        user_idx = self.user_id_to_index[user_id]\n",
    "        user_vector = self.user_item_matrix[user_idx].toarray().flatten()\n",
    "        rated_items = user_vector.nonzero()[0]\n",
    "        \n",
    "        if len(rated_items) == 0:\n",
    "            return None\n",
    "        \n",
    "        predictions = np.zeros(self.user_item_matrix.shape[1])\n",
    "        \n",
    "        for item_idx in range(self.user_item_matrix.shape[1]):\n",
    "            if user_vector[item_idx] == 0:\n",
    "                similar_items = self.item_similarity_matrix[item_idx].indices\n",
    "                rated_similar_items = np.intersect1d(similar_items, rated_items)\n",
    "                \n",
    "                if len(rated_similar_items) > 0:\n",
    "                    similarities = self.item_similarity_matrix[item_idx, rated_similar_items].toarray().flatten()\n",
    "                    ratings = user_vector[rated_similar_items]\n",
    "                    \n",
    "                    if np.sum(similarities) > 0:\n",
    "                        predictions[item_idx] = np.sum(similarities * ratings) / np.sum(similarities)\n",
    "        \n",
    "        top_indices = np.argsort(predictions)[::-1][:top_n]\n",
    "        results = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if predictions[idx] > 0:\n",
    "                results.append({\n",
    "                    'product_id': self.index_to_item_id[idx],\n",
    "                    'predicted_rating': predictions[idx],\n",
    "                    'rank': len(results) + 1\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_knn_predict(self, user_ids, top_n=10):\n",
    "        \"\"\"–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\"\"\"\n",
    "        all_predictions = []\n",
    "        \n",
    "        for user_id in user_ids:\n",
    "            predictions = self.knn_predict(user_id, top_n)\n",
    "            if predictions:\n",
    "                for pred in predictions:\n",
    "                    all_predictions.append({\n",
    "                        'user_id': user_id,\n",
    "                        'product_id': pred['product_id'],\n",
    "                        'rating_knn': pred['predicted_rating'],\n",
    "                        'rank': pred['rank']\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf7c6b09-7719-4bd5-9717-1a813bba53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RecommenderOptimizer:\n",
    "    def __init__(self, rating_df):\n",
    "        self.rating_df = rating_df\n",
    "        self.best_knn_params = {'k': 250, 'method': 'cosine'}\n",
    "        self.best_svd_params = {'n_factors': 480}\n",
    "        self.best_hybrid_weights = (0.6, 0.4)\n",
    "        self.data = None\n",
    "        self.trainset = None\n",
    "        self.testset = None\n",
    "        self.knn_recommender = EnhancedKNNRecommender()\n",
    "        self.compressed_df = None\n",
    "    \n",
    "    def prepare_fast_optimization(self):\n",
    "        \"\"\"–ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\"\"\"\n",
    "        logger.info(\"üöÄ –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...\")\n",
    "        \n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (10% –¥–∞–Ω–Ω—ã—Ö)\n",
    "        sample_size = min(500000, len(self.rating_df))\n",
    "        optimization_df = self.rating_df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        # –°–∂–∏–º–∞–µ–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "        self.compressed_df = self.knn_recommender.compress_dataset(\n",
    "            optimization_df, \n",
    "            min_user_interactions=1, \n",
    "            min_item_interactions=1\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"–ü–æ–¥–≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {len(optimization_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "        logger.info(f\"–°–∂–∞—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(self.compressed_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è KNN\n",
    "        self.knn_recommender.create_sparse_matrix(self.compressed_df)\n",
    "        \n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Surprise\n",
    "        self.prepare_surprise_data(test_size=0.2)\n",
    "        \n",
    "        logger.info(\"‚úÖ –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "    \n",
    "    def prepare_surprise_data(self, test_size=0.2):\n",
    "        \n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è Surprise —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é\n",
    "        surprise_sample = self.rating_df.sample(min(100000, len(self.rating_df)), random_state=42)\n",
    "        \n",
    "        surprise_df = surprise_sample.rename(columns={\n",
    "            'user_id': 'userID',\n",
    "            'product_id': 'itemID'\n",
    "        })\n",
    "        \n",
    "        min_rating = surprise_df['rating'].min()\n",
    "        max_rating = surprise_df['rating'].max()\n",
    "        \n",
    "        reader = Reader(rating_scale=(min_rating, max_rating))\n",
    "        self.data = Dataset.load_from_df(surprise_df[['userID', 'itemID', 'rating']], reader)\n",
    "        \n",
    "        self.trainset, self.testset = surprise_train_test_split(self.data, test_size=test_size, random_state=42)\n",
    "        return True\n",
    "        \n",
    "    def optimize_svd(self, n_factors_list=[80, 120, 200, 360]):\n",
    "                \n",
    "        logger.info(\"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVD –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...\")\n",
    "        \n",
    "        best_rmse = float('inf')\n",
    "        best_n_factors = 50\n",
    "        \n",
    "        for n_factors in tqdm(n_factors_list, desc=\"SVD Optimization\"):\n",
    "            try:\n",
    "                algo = SVD(n_factors=n_factors, biased=True, random_state=999, verbose=False)\n",
    "                \n",
    "                algo.fit(self.trainset)\n",
    "                predictions = algo.test(self.testset)\n",
    "                \n",
    "                rmse = np.sqrt(np.mean([(pred.r_ui - pred.est) ** 2 for pred in predictions]))\n",
    "                \n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_n_factors = n_factors\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"–û—à–∏–±–∫–∞ –¥–ª—è n_factors={n_factors}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_svd_params = {'n_factors': best_n_factors}\n",
    "        logger.info(f\"üéØ –õ—É—á—à–∏–µ SVD –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: n_factors={best_n_factors}, RMSE={best_rmse:.4f}\")\n",
    "        \n",
    "        return best_n_factors, best_rmse\n",
    "\n",
    "    def optimize_knn_on_compressed_data(self, k_list=[20, 50, 100]):\n",
    "        \"\"\"KNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∂–∞—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\"\"\"\n",
    "        logger.info(\"KNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∂–∞—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...\")\n",
    "        \n",
    "        if self.compressed_df is None:\n",
    "            self.compressed_df = self.knn_recommender.compress_dataset(\n",
    "                self.rating_df, min_user_interactions=1, min_item_interactions=1\n",
    "            )\n",
    "        \n",
    "        self.knn_recommender.create_sparse_matrix(self.compressed_df)\n",
    "        \n",
    "        best_rmse = float('inf')\n",
    "        best_k = 50\n",
    "        \n",
    "        for k in tqdm(k_list, desc=\"KNN Optimization on compressed data\"):\n",
    "            try:\n",
    "                self.knn_recommender.build_similarity_matrix(k=k)\n",
    "                \n",
    "                # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ\n",
    "                sample_users = self.compressed_df['user_id'].unique()[:50]\n",
    "                knn_predictions = self.knn_recommender.batch_knn_predict(sample_users, 10)\n",
    "                \n",
    "                if len(knn_predictions) > 0:\n",
    "                    rmse = self._evaluate_knn_predictions(knn_predictions)\n",
    "                    \n",
    "                    if rmse < best_rmse and rmse != float('inf'):\n",
    "                        best_rmse = rmse\n",
    "                        best_k = k\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"–û—à–∏–±–∫–∞ –¥–ª—è k={k}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_knn_params['k'] = best_k\n",
    "        logger.info(f\"üéØ –õ—É—á—à–∏–π KNN: k={best_k}, RMSE={best_rmse:.4f}\")\n",
    "        return best_k, best_rmse\n",
    "\n",
    "    def _evaluate_knn_predictions(self, knn_predictions):\n",
    "        \"\"\"–û—Ü–µ–Ω–∫–∞ KNN –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π\"\"\"\n",
    "        if len(knn_predictions) == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º inner join –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è\n",
    "        merged = knn_predictions.merge(\n",
    "            self.rating_df, \n",
    "            on=['user_id', 'product_id'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        if len(merged) == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –µ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "        valid_data = merged.dropna(subset=['rating', 'rating_knn'])\n",
    "        if len(valid_data) == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        return np.sqrt(mean_squared_error(valid_data['rating'], valid_data['rating_knn']))\n",
    "\n",
    "    def optimize_hybrid_weights(self):\n",
    "        \"\"\"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏\"\"\"\n",
    "        logger.info(\"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –≤–µ—Å–æ–≤...\")\n",
    "        \n",
    "        best_score = 0\n",
    "        best_weights = (0.6, 0.4)  # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ä–∞–≤–Ω—ã—Ö –≤–µ—Å–æ–≤\n",
    "        \n",
    "        for svd_weight in [0.4, 0.5, 0.6, 0.7]:\n",
    "            knn_weight = 1.0 - svd_weight\n",
    "            \n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏\n",
    "            sample_users = self.compressed_df['user_id'].unique()[:20]\n",
    "            hybrid_score = self._evaluate_hybrid_weights(sample_users, svd_weight, knn_weight)\n",
    "            \n",
    "            if hybrid_score > best_score:\n",
    "                best_score = hybrid_score\n",
    "                best_weights = (svd_weight, knn_weight)\n",
    "        \n",
    "        self.best_hybrid_weights = best_weights\n",
    "        logger.info(f\"üéØ –õ—É—á—à–∏–µ –≤–µ—Å–∞: SVD={best_weights[0]}, KNN={best_weights[1]}\")\n",
    "        return best_weights\n",
    "\n",
    "    def _evaluate_hybrid_weights(self, user_ids, svd_weight, knn_weight):\n",
    "        \"\"\"–û—Ü–µ–Ω–∫–∞ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –≤–µ—Å–æ–≤\"\"\"\n",
    "        _, svd_algo = self.train_final_models()\n",
    "        knn_predictions = self.knn_recommender.batch_knn_predict(user_ids, 10)\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for user_id in user_ids:\n",
    "            try:\n",
    "                user_inner_id = svd_algo.trainset.to_inner_uid(str(user_id))\n",
    "                user_bias = svd_algo.bu[user_inner_id]\n",
    "                user_factors = svd_algo.pu[user_inner_id]\n",
    "                svd_ratings = svd_algo.bi + user_bias + np.dot(svd_algo.qi, user_factors)\n",
    "                \n",
    "                user_knn = knn_predictions[knn_predictions['user_id'] == user_id]\n",
    "                \n",
    "                hybrid_scores = []\n",
    "                for item_inner_id, svd_rating in enumerate(svd_ratings):\n",
    "                    try:\n",
    "                        item_id = int(svd_algo.trainset.to_raw_iid(item_inner_id))\n",
    "                        knn_rating = 0\n",
    "                        \n",
    "                        knn_match = user_knn[user_knn['product_id'] == item_id]\n",
    "                        if not knn_match.empty:\n",
    "                            knn_rating = knn_match['rating_knn'].values[0]\n",
    "                        \n",
    "                        hybrid_rating = (svd_weight * svd_rating + \n",
    "                                       knn_weight * knn_rating if knn_rating > 0 else svd_rating)\n",
    "                        hybrid_scores.append(hybrid_rating)\n",
    "                        \n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if hybrid_scores:\n",
    "                    scores.append(max(hybrid_scores))\n",
    "                    \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return np.mean(scores) if scores else 0\n",
    "\n",
    "    def train_final_models(self):\n",
    "                  \n",
    "        logger.info(\"–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π SVD –º–æ–¥–µ–ª–∏...\")\n",
    "        \n",
    "        full_trainset = self.data.build_full_trainset()\n",
    "        \n",
    "        svd_algo = SVD(n_factors=self.best_svd_params['n_factors'], biased=True, \n",
    "                      random_state=999, verbose=False)\n",
    "        svd_algo.fit(full_trainset)\n",
    "        \n",
    "        return None, svd_algo\n",
    "    \n",
    "    def create_simple_submission(self, n_recommendations=10):\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ submission —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤\"\"\"\n",
    "        logger.info(\"–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...\")\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç–æ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º\n",
    "        submission_df = (\n",
    "            self.rating_df\n",
    "            .sort_values(['user_id', 'rating'], ascending=[True, False])\n",
    "            .groupby('user_id')\n",
    "            .head(n_recommendations)\n",
    "            .groupby('user_id')['product_id']\n",
    "            .apply(lambda x: ' '.join(map(str, x)))\n",
    "            .reset_index()\n",
    "            .rename(columns={'product_id': 'product_id'})\n",
    "        )\n",
    "        \n",
    "        submission_df.to_csv('submission.csv', index=False)\n",
    "        logger.info(f\"‚úÖ Submission —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {len(submission_df)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\")\n",
    "        \n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7c3c528-4d39-4685-9af5-6d7c24d2f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 13:33:18,501 - INFO - üöÄ –ó–∞–ø—É—Å–∫ OurDataWinnerV20...\n",
      "2025-08-27 13:33:18,502 - INFO - –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
      "2025-08-27 13:33:18,502 - INFO - –ê–≤—Ç–æ–ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö...\n",
      "2025-08-27 13:33:18,503 - INFO - –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª transactions: data/transactions.csv\n",
      "2025-08-27 13:33:18,503 - INFO - üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "2025-08-27 13:33:18,504 - INFO - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è data/transactions.csv...\n",
      "2025-08-27 13:33:18,508 - INFO - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ user_id -> user_id\n",
      "2025-08-27 13:33:18,509 - INFO - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ product_id -> product_id\n",
      "2025-08-27 13:33:18,509 - INFO - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ order_id -> order_id\n",
      "2025-08-27 13:33:18,510 - INFO - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ order_number -> order_number\n",
      "2025-08-27 13:33:18,510 - INFO - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ reordered -> reordered\n",
      "2025-08-27 13:33:18,511 - INFO - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ add_to_cart_order -> add_to_cart_order\n",
      "2025-08-27 13:33:18,511 - INFO - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ order_dow -> order_dow\n",
      "2025-08-27 13:33:18,512 - INFO - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ order_hour_of_day -> order_hour_of_day\n",
      "2025-08-27 13:33:18,512 - INFO - –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫: {'user_id': 'user_id', 'product_id': 'product_id', 'order_id': 'order_id', 'order_number': 'order_number', 'reordered': 'reordered', 'add_to_cart_order': 'add_to_cart_order', 'order_dow': 'order_dow', 'order_hour_of_day': 'order_hour_of_day'}\n",
      "2025-08-27 13:33:18,512 - INFO - –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "2025-08-27 13:33:26,883 - INFO - ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 26,408,073 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π\n",
      "2025-08-27 13:33:27,164 - INFO - –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∏—á...\n",
      "2025-08-27 13:33:36,443 - INFO - ‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã\n",
      "2025-08-27 13:33:36,464 - INFO - –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤...\n",
      "2025-08-27 13:33:36,757 - INFO - ‚úÖ –ú–∞–ø–ø–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã: 100000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, 49465 —Ç–æ–≤–∞—Ä–æ–≤\n",
      "2025-08-27 13:33:36,758 - INFO - –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä—ã–π —Ä–∞—Å—á–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ø–æ 12 —Ñ–∏—á–∞–º...\n",
      "2025-08-27 13:33:36,759 - INFO - –†–∞—Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\n",
      "2025-08-27 13:36:02,860 - INFO - –°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–æ–≤: 9459065 –∑–∞–ø–∏—Å–µ–π\n",
      "2025-08-27 13:36:09,621 - INFO - ‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\n",
      "2025-08-27 13:36:09,623 - INFO - ‚úÖ –°–æ–∑–¥–∞–Ω —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: 9459065 –∑–∞–ø–∏—Å–µ–π\n",
      "2025-08-27 13:36:09,623 - INFO - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤:\n",
      "2025-08-27 13:36:09,624 - INFO -   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: 9,459,065\n",
      "2025-08-27 13:36:09,675 - INFO -   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: 100000\n",
      "2025-08-27 13:36:09,722 - INFO -   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: 49465\n",
      "2025-08-27 13:36:09,742 - INFO -   –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: 5.571\n",
      "2025-08-27 13:36:09,759 - INFO -   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: 2.158\n",
      "2025-08-27 13:36:09,779 - INFO -   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: 63.268\n",
      "2025-08-27 13:36:09,935 - INFO -   –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: 4.291\n",
      "2025-08-27 13:36:10,024 - INFO - üéØ –ë—ã—Å—Ç—Ä–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVD + KNN...\n",
      "2025-08-27 13:36:10,025 - INFO - üöÄ –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...\n",
      "2025-08-27 13:36:10,474 - INFO - –°–∂–∞—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...\n",
      "2025-08-27 13:36:10,502 - INFO - –°–∂–∞—Ç–∏–µ: 500000 ‚Üí 500000 –∑–∞–ø–∏—Å–µ–π (92641 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, 32866 —Ç–æ–≤–∞—Ä–æ–≤)\n",
      "2025-08-27 13:36:10,503 - INFO - –ü–æ–¥–≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: 500000 –∑–∞–ø–∏—Å–µ–π\n",
      "2025-08-27 13:36:10,504 - INFO - –°–∂–∞—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç: 500000 –∑–∞–ø–∏—Å–µ–π\n",
      "2025-08-27 13:36:10,505 - INFO - –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä...\n",
      "2025-08-27 13:36:24,520 - INFO - –ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: 92641 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π √ó 32866 —Ç–æ–≤–∞—Ä–æ–≤\n",
      "2025-08-27 13:36:25,045 - INFO - ‚úÖ –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n",
      "2025-08-27 13:36:25,047 - INFO - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVD –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...\n",
      "SVD Optimization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.48s/it]\n",
      "2025-08-27 13:36:30,966 - INFO - üéØ –õ—É—á—à–∏–µ SVD –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: n_factors=80, RMSE=2.8023\n",
      "2025-08-27 13:36:30,978 - INFO - KNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∂–∞—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...\n",
      "2025-08-27 13:36:30,980 - INFO - –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä...\n",
      "2025-08-27 13:36:44,856 - INFO - –ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: 92641 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π √ó 32866 —Ç–æ–≤–∞—Ä–æ–≤\n",
      "KNN Optimization on compressed data:   0%|                                                       | 0/3 [00:00<?, ?it/s]2025-08-27 13:36:44,907 - INFO - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ (method=cosine, k=20)...\n",
      "2025-08-27 13:36:56,093 - INFO - –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞\n",
      "KNN Optimization on compressed data:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 1/3 [01:38<03:16, 98.28s/it]2025-08-27 13:38:23,184 - INFO - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ (method=cosine, k=50)...\n",
      "2025-08-27 13:38:38,869 - INFO - –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞\n",
      "KNN Optimization on compressed data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2/3 [03:29<01:45, 105.64s/it]2025-08-27 13:40:13,979 - INFO - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ (method=cosine, k=100)...\n",
      "2025-08-27 13:40:37,811 - INFO - –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞\n",
      "KNN Optimization on compressed data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [05:35<00:00, 111.92s/it]\n",
      "2025-08-27 13:42:20,661 - INFO - üéØ –õ—É—á—à–∏–π KNN: k=50, RMSE=2.6967\n",
      "2025-08-27 13:42:20,663 - INFO - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –≤–µ—Å–æ–≤...\n",
      "2025-08-27 13:42:20,671 - INFO - –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π SVD –º–æ–¥–µ–ª–∏...\n",
      "2025-08-27 13:43:01,458 - INFO - –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π SVD –º–æ–¥–µ–ª–∏...\n",
      "2025-08-27 13:43:41,746 - INFO - –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π SVD –º–æ–¥–µ–ª–∏...\n",
      "2025-08-27 13:44:22,425 - INFO - –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π SVD –º–æ–¥–µ–ª–∏...\n",
      "2025-08-27 13:45:03,224 - INFO - üéØ –õ—É—á—à–∏–µ –≤–µ—Å–∞: SVD=0.6, KNN=0.4\n",
      "2025-08-27 13:45:03,225 - INFO - ‚úÖ –ì–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n",
      "2025-08-27 13:45:03,226 - INFO -    SVD: n_factors=80, RMSE=2.8023\n",
      "2025-08-27 13:45:03,227 - INFO -    KNN: k=50, RMSE=2.6967\n",
      "2025-08-27 13:45:03,227 - INFO -    –í–µ—Å–∞: SVD=0.6, KNN=0.4\n",
      "2025-08-27 13:45:03,228 - INFO - –°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...\n",
      "2025-08-27 13:45:03,228 - INFO - –°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...\n",
      "2025-08-27 13:45:15,513 - INFO - ‚úÖ Submission —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: 100000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
      "2025-08-27 13:45:15,515 - INFO - ‚úÖ Submission —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!\n",
      "2025-08-27 13:45:15,515 - INFO - üéâ –ü—Ä–æ–≥—Ä–∞–º–º–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logger.info(\"üöÄ –ó–∞–ø—É—Å–∫ OurDataWinnerV20...\")\n",
    "    \n",
    "    recommender = OurDataWinnerV20()\n",
    "    \n",
    "    # –ê–≤—Ç–æ–ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤\n",
    "    recommender.auto_detect_data_files()\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    if not recommender.load_and_preprocess_data():\n",
    "        logger.error(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!\")\n",
    "        return\n",
    "    \n",
    "    # –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n",
    "    if not recommender.optimize_recommendations():\n",
    "        logger.error(\"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏!\")\n",
    "        return\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞\n",
    "    if not recommender.create_submission_file():\n",
    "        logger.error(\"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å submission —Ñ–∞–π–ª!\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"üéâ –ü—Ä–æ–≥—Ä–∞–º–º–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (instacart_env)",
   "language": "python",
   "name": "instacart_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
